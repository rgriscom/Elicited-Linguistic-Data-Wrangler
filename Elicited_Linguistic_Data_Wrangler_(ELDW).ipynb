{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Elicited Linguistic Data Wrangler (ELDW).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4pfwKcwLKhs"
      },
      "source": [
        "#Last updated: 2020-11-20\n",
        "\n",
        "\n",
        "#Author: Richard Griscom\n",
        "#Contact: rgriscom@gmail.com\n",
        "#Description: This script is designed to enable linguists to quickly make their text data time-aligned, searchable, and archivable.\n",
        "#NOTE: THIS SCRIPT IS STILL IN DEVELOPMENT.\n",
        "* It assumes that you have a .wav audio recording, a tab-delimited .tsv file with two or more columns of text data (e.g. transcription and translation), and a .TextGrid file that includes segment timecode data (either manually created or automatically through \"Annotate to Silences...\").\n",
        "* It combines the text data and timecode data and outputs in three formats: .EAF, .TextGrid, and .TXT.  \n",
        "\n",
        "ISO 639 Language Codes: https://iso639-3.sil.org/code_tables/639/data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNbOn2pELLIw"
      },
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "TODO: \n",
        "-Reformat nested dictionaries as dataframes\n",
        "-Move some remaining form elements to ipywidgets depending on the source data types\n",
        "-Add .eaf text reading\n",
        "-Add .TextGrid text reading\n",
        "-Add .flextext text reading\n",
        "-Add .flextext (Multiple texts) as data source AND output (it essentially compiles multiple texts into a single flextext file)\n",
        "\n",
        "\n",
        "Note: for .TextGrid and .eaf text data sources, the tier titles, language codes, and types still need to be manually specified (just like with .csv) in order for .flextext output to be possible\n",
        "Note: The script will not produce .TextGrid or .eaf files if no timecode data, and it will not produce .flextext if no text data\n",
        "Note: multiple translations and the REFID need to be enabled within flex (Tools > Configure > Interlinear)\n",
        "Note: Phrase/Annotation-level notes are not accepted by FLEx\n",
        "Note: If sounding_interval_text is set to nothing, then any non-empty segment will be used for text\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "import uuid\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from __future__ import print_function\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "\n",
        "#@title Settings\n",
        "Operating_System = 'Linux' #@param [\"Linux\", \"MacOS\", \"Windows\"]\n",
        "Text_Data_Source = '.csv' #@param [\".csv\", \".TextGrid\", \".eaf\", \".flextext\", \"None\"]\n",
        "Timecode_Data_Source = '.TextGrid' #@param [\".csv\", \".TextGrid\", \".eaf\", \".flextext\",\"None\"]\n",
        "Output_CSV = True #@param {type:\"boolean\"}\n",
        "Output_TextGrid = True #@param {type:\"boolean\"}\n",
        "Output_EAF = True #@param {type:\"boolean\"}\n",
        "Output_Flextext = True #@param {type:\"boolean\"}\n",
        "#Number_of_Columns = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "Expand_Time_Segments = False\n",
        "Amount_to_Expand = 10\n",
        "Sounding_Interval_Text = '' #@param {type:\"string\"}\n",
        "CSV_Timecode_Format = 'Seconds' #@param [\"Seconds\", \"Milliseconds\"]\n",
        "Add_REFID = True #@param {type:\"boolean\"}\n",
        "\n",
        "### Text data settings\n",
        "if Text_Data_Source == '.csv':\n",
        "  column_names = []\n",
        "  column_types = []\n",
        "  column_lgs = []\n",
        "  number_of_columns = 3\n",
        "  column_visibility = [1,1,1,0,0,0,0,0,0,0]\n",
        "  out = widgets.Output()\n",
        "  display(out)\n",
        "  CSV_Delimiter = \"Comma\"\n",
        "  column_name_entry_list = []\n",
        "  column_type_entry_list = []\n",
        "  column_lg_entry_list = []\n",
        "  First_Row_Is_Column_Names = False\n",
        "  j = 0\n",
        "  @out.capture()\n",
        "  def name_handle_submit(change):\n",
        "    try:\n",
        "      column_names[int(change['owner'].description.split(' name:')[0])-1] = change['new']\n",
        "    except:\n",
        "      column_names.append(change['new'])\n",
        "\n",
        "  @out.capture()\n",
        "  def type_handle_submit(change):\n",
        "    try:\n",
        "      column_types[int(change['owner'].description.split(' type:')[0])-1] = change['new']\n",
        "    except:\n",
        "      column_types.append(change['new'])\n",
        "\n",
        "  @out.capture()\n",
        "  def lg_handle_submit(change):\n",
        "    try:\n",
        "      column_lgs[int(change['owner'].description.split(' ISO 639:')[0])-1] = change['new']\n",
        "    except:\n",
        "      column_lgs.append(change['new'])\n",
        "\n",
        "  @out.capture()\n",
        "  def slider_handle_submit(change):  \n",
        "    global number_of_columns\n",
        "    global First_Row_Is_Column_Names \n",
        "    number_of_columns = change['new']\n",
        "    for i in range(10):\n",
        "      if (i+1) <= number_of_columns:\n",
        "        if column_type_entry_list[i].layout.display == 'none':\n",
        "          column_type_entry_list[i].layout.display = 'flex'\n",
        "        if column_lg_entry_list[i].layout.display == 'none':\n",
        "          column_lg_entry_list[i].layout.display = 'flex'\n",
        "        if column_name_entry_list[i].layout.display == 'none' and First_Row_Is_Column_Names == False:\n",
        "          column_name_entry_list[i].layout.display = 'flex'\n",
        "      if (i+1) > number_of_columns:\n",
        "        if column_type_entry_list[i].layout.display == 'flex':\n",
        "          column_type_entry_list[i].layout.display = 'none'\n",
        "        if column_lg_entry_list[i].layout.display == 'flex':\n",
        "          column_lg_entry_list[i].layout.display = 'none'\n",
        "        if column_name_entry_list[i].layout.display == 'flex':\n",
        "          column_name_entry_list[i].layout.display = 'none'\n",
        "  @out.capture()\n",
        "  def delimiter_handle_submit(change):\n",
        "    global CSV_Delimiter\n",
        "    CSV_Delimiter = change['new']\n",
        "  \n",
        "  @out.capture()\n",
        "  def first_row_handle_submit(change):\n",
        "    global First_Row_Is_Column_Names\n",
        "    First_Row_Is_Column_Names = change['new']\n",
        "    for i in range(10):\n",
        "      if First_Row_Is_Column_Names == True:\n",
        "          column_name_entry_list[i].layout.display = 'none'\n",
        "      if First_Row_Is_Column_Names == False and (i+1) <= number_of_columns:\n",
        "          column_name_entry_list[i].layout.display = 'flex'\n",
        "\n",
        "  column_slider = widgets.IntSlider(\n",
        "      value=3,\n",
        "      min=1,\n",
        "      max=10,\n",
        "      step=1,\n",
        "      description='Column_slider',\n",
        "      disabled=False,\n",
        "      continuous_update=True,\n",
        "      orientation='horizontal',\n",
        "      readout=True,\n",
        "      readout_format='d'\n",
        "  )\n",
        "  display(column_slider)  \n",
        "  csv_delimiter_dropdown = widgets.Dropdown(\n",
        "      options=['Comma','Tab'],\n",
        "      value='Comma',\n",
        "      description=\"CSV Delimiter: \",\n",
        "      disabled=False)\n",
        "  display(csv_delimiter_dropdown)\n",
        "  First_Row_Is_Column_Names_checkbox = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='First row of CSV is column titles?',\n",
        "    disabled=False,\n",
        "    indent=False\n",
        "  )\n",
        "  display(First_Row_Is_Column_Names_checkbox)\n",
        "  for i in range(10):\n",
        "    if First_Row_Is_Column_Names == False:\n",
        "      column_name_entry_list.append(widgets.Text(\n",
        "        value='',\n",
        "        placeholder='Name',\n",
        "        description=str(i+1) + \" name:\",\n",
        "        continuous_update=True,\n",
        "        disabled=False))\n",
        "      display(column_name_entry_list[i])\n",
        "      if (i+1) > number_of_columns:\n",
        "        column_name_entry_list[i].layout.display = 'none'\n",
        "      else:\n",
        "        column_name_entry_list[i].layout.display = 'flex'\n",
        "    column_type_entry_list.append(widgets.Dropdown(\n",
        "      options=['Select One','Transcription', 'Translation', 'Notes'],\n",
        "      value='Select One',\n",
        "      description=str(i+1) + \" type:\",\n",
        "      disabled=False))\n",
        "    display(column_type_entry_list[i])\n",
        "    if (i+1) > number_of_columns:\n",
        "      column_type_entry_list[i].layout.display = 'none'\n",
        "    else:\n",
        "      column_type_entry_list[i].layout.display = 'flex'\n",
        "    column_lg_entry_list.append(widgets.Text(\n",
        "      value='',\n",
        "      placeholder='ISO 639 language code',\n",
        "      description=str(i+1) + \" ISO 639:\",\n",
        "      continuous_update=True,\n",
        "      disabled=False))\n",
        "    display(column_lg_entry_list[i])\n",
        "    if (i+1) > number_of_columns:\n",
        "      column_lg_entry_list[i].layout.display = 'none'\n",
        "    else:\n",
        "      column_lg_entry_list[i].layout.display = 'flex'\n",
        "\n",
        "  column_slider.observe(slider_handle_submit, names='value')\n",
        "  csv_delimiter_dropdown.observe(delimiter_handle_submit, names='value')\n",
        "  First_Row_Is_Column_Names_checkbox.observe(first_row_handle_submit, names='value')\n",
        "\n",
        "  for i in range(number_of_columns):\n",
        "    if First_Row_Is_Column_Names == False:\n",
        "      column_name_entry_list[i].observe(name_handle_submit, names='value')\n",
        "    column_type_entry_list[i].observe(type_handle_submit, names='value')\n",
        "    column_lg_entry_list[i].observe(lg_handle_submit, names='value')\n",
        "\n",
        "\n",
        "###Time code settings\n",
        "if Timecode_Data_Source != 'None':\n",
        "  @out.capture()\n",
        "  def time_segments_checkbox_handle_submit(change):\n",
        "    global Expand_Time_Segments\n",
        "    Expand_Time_Segments = change['new']\n",
        "    if Expand_Time_Segments == True:\n",
        "      Amount_to_Expand_slider.layout.display = 'flex'\n",
        "    if Expand_Time_Segments == False:\n",
        "      Amount_to_Expand_slider.layout.display = 'none'\n",
        "\n",
        "  @out.capture()\n",
        "  def amount_to_expand_slider_handle_submit(change):\n",
        "    global Amount_to_Expand\n",
        "    Amount_to_Expand = change['new']\n",
        "\n",
        "  Expand_Time_Segments_checkbox = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Expand duration of time segments',\n",
        "    disabled=False,\n",
        "    indent=False\n",
        "  )\n",
        "  display(Expand_Time_Segments_checkbox)\n",
        "  Amount_to_Expand_slider = widgets.IntSlider(\n",
        "      value=10,\n",
        "      min=0,\n",
        "      max=100,\n",
        "      step=10,\n",
        "      description='Amount to expand timecode (ms)',\n",
        "      disabled=False,\n",
        "      continuous_update=True,\n",
        "      orientation='horizontal',\n",
        "      readout=True,\n",
        "      readout_format='d'\n",
        "  )\n",
        "  display(Amount_to_Expand_slider)\n",
        "  Amount_to_Expand_slider.layout.display = 'none'\n",
        "  Amount_to_Expand_slider.observe(amount_to_expand_slider_handle_submit, names='value')\n",
        "  Expand_Time_Segments_checkbox.observe(time_segments_checkbox_handle_submit, names='value')\n",
        "\n",
        "\n",
        "   \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fumRor_RYHl"
      },
      "source": [
        "#@title\n",
        "\"\"\"\n",
        "print(\"Names:\")\n",
        "print(column_names)\n",
        "print(\"Types:\")\n",
        "print(column_types)\n",
        "print(\"Lgs:\")\n",
        "print(column_lgs)\n",
        "\"\"\"\n",
        "try:\n",
        "  !rm -rf Output\n",
        "  !mkdir Output\n",
        "except:\n",
        "  !mkdir Output\n",
        "from google.colab import files\n",
        "import datetime\n",
        "now = datetime.datetime.now()\n",
        "uploaded = files.upload()     \n",
        "wrong_file_format = 0\n",
        "\n",
        "if First_Row_Is_Column_Names == True or Text_Data_Source != \".csv\":\n",
        "  column_names = []\n",
        "\n",
        "\n",
        "\n",
        "list_of_files = []\n",
        "for fn in uploaded.keys():\n",
        "  filename_stem = fn.split(\".\")[0]\n",
        "  if filename_stem not in list_of_files:\n",
        "    list_of_files.append(filename_stem)\n",
        "\n",
        "#For each file in the list of files\n",
        "for fn in list_of_files:\n",
        "  new_csv_name = \"Output/\" + fn + \".csv\"\n",
        "  new_textgrid_name = \"Output/\" + fn + \".TextGrid\"\n",
        "  new_flextext_name = \"Output/\" + fn + \".flextext\"\n",
        "  new_eaf_name = \"Output/\" + fn + \".eaf\"\n",
        "  tsv_dict = {}  \n",
        "\n",
        "  #If text source is being used\n",
        "  if Text_Data_Source != None:\n",
        "    #Determine the exact extension\n",
        "    if fn + Text_Data_Source in uploaded.keys():\n",
        "      extension = Text_Data_Source\n",
        "    if fn + Text_Data_Source.upper() in uploaded.keys():\n",
        "      extension = Text_Data_Source.upper()\n",
        "    if fn + Text_Data_Source.lower() in uploaded.keys():\n",
        "      extension = Text_Data_Source.lower()\n",
        "    print(\"Processing text from file: \" + fn + extension)\n",
        "    \n",
        "    with open(fn + extension) as text_source:\n",
        "      annotation_counter = 1\n",
        "      names_check = False\n",
        "      #.CSV text extraction process\n",
        "      if Text_Data_Source == \".csv\":\n",
        "        print(\".csv process\")\n",
        "        if CSV_Delimiter == \"Comma\":\n",
        "          delim = \",\"\n",
        "        if CSV_Delimiter == \"Tab\":\n",
        "          delim = \"\\t\"\n",
        "        column_data = []\n",
        "        for line in text_source:\n",
        "          temp = line.split('\\n')\n",
        "          rows = temp[0].split(delim)\n",
        "          if First_Row_Is_Column_Names == True and names_check == False:\n",
        "            for i in rows:\n",
        "              column_names.append(i)\n",
        "            total_columns = len(column_names)\n",
        "            print(\"Column names from first row: \")\n",
        "            print(column_names)\n",
        "            names_check = True\n",
        "          else:\n",
        "            tsv_dict[annotation_counter] = {}\n",
        "            tsv_dict[annotation_counter]['REFID'] = filename_stem + \"_\" + str(annotation_counter)\n",
        "            counter = 1\n",
        "            while counter < (total_columns + 1):\n",
        "              try:\n",
        "                tsv_dict[annotation_counter][column_names[(counter - 1)]] = rows[(counter - 1)]\n",
        "              except IndexError:\n",
        "                tsv_dict[annotation_counter][column_names[(counter - 1)]] = \"\"                                  \n",
        "              counter += 1\n",
        "            annotation_counter = annotation_counter + 1\n",
        "        print(\"Column names: \")\n",
        "        print(column_names)\n",
        "\n",
        "      #.TextGrid text extraction process\n",
        "      if Text_Data_Source == \".TextGrid\":\n",
        "        print(\".TextGrid text process\")\n",
        "        tier_counter = 0\n",
        "        annotation_counter = 1\n",
        "        for line in text_source:\n",
        "          if \"name = \\\"\" in line:\n",
        "            column_names.append(line.split('name = \\\"')[1].split(\"/\")[0])\n",
        "            tier_counter += 1\n",
        "            annotation_counter = 1\n",
        "          if \"text = \\\"\" in line and \"text = \\\"\\\"\" not in line:\n",
        "            if tier_counter == 1:\n",
        "              tsv_dict[annotation_counter] = {}\n",
        "              tsv_dict[annotation_counter]['REFID'] = filename_stem + \"_\" + str(annotation_counter)\n",
        "            tsv_dict[annotation_counter][column_names[(tier_counter - 1)]] = line.split('text = \\\"')[1].split(\"/\")[0]\n",
        "            annotation_counter += 1\n",
        "        print(\"Column names: \")\n",
        "        print(column_names)\n",
        "        \"\"\"\n",
        "        column_data = []\n",
        "        \n",
        "          if \n",
        "          temp = line.split('\\n')\n",
        "          \n",
        "          #Extract the text from each segment\n",
        "          \n",
        "            print(\"Column names from first row: \")\n",
        "            print(column_names)\n",
        "          else:\n",
        "            tsv_dict[annotation_counter] = {}\n",
        "            tsv_dict[annotation_counter]['REFID'] = filename_stem + \"_\" + str(annotation_counter)\n",
        "            counter = 1\n",
        "            while counter < (total_columns + 1):\n",
        "              try:\n",
        "                tsv_dict[annotation_counter][column_names[(counter - 1)]] = rows[(counter - 1)]\n",
        "              except IndexError:\n",
        "                tsv_dict[annotation_counter][column_names[(counter - 1)]] = \"\"                                  \n",
        "              counter += 1\n",
        "            annotation_counter = annotation_counter + 1\n",
        "       \"\"\"\n",
        "\n",
        "      #.flextext text extraction process\n",
        "      if Text_Data_Source == \".flextext\": \n",
        "        print(\".flextext text process\")\n",
        "\n",
        "        \"\"\"\n",
        "        column_data = []\n",
        "        for line in text_source:\n",
        "          temp = line.split('\\n')\n",
        "          \n",
        "          #Extract all of the text info, inclusind interlinearized text (this will take a while to code!)\n",
        "          \n",
        "            print(\"Column names from first row: \")\n",
        "            print(column_names)\n",
        "          else:\n",
        "            tsv_dict[annotation_counter] = {}\n",
        "            tsv_dict[annotation_counter]['REFID'] = filename_stem + \"_\" + str(annotation_counter)\n",
        "            counter = 1\n",
        "            while counter < (total_columns + 1):\n",
        "              try:\n",
        "                tsv_dict[annotation_counter][column_names[(counter - 1)]] = rows[(counter - 1)]\n",
        "              except IndexError:\n",
        "                tsv_dict[annotation_counter][column_names[(counter - 1)]] = \"\"                                  \n",
        "              counter += 1\n",
        "            annotation_counter = annotation_counter + 1\n",
        "            \"\"\"\n",
        "      #.eaf text extraction process\n",
        "      if Text_Data_Source == \".eaf\":\n",
        "        print(\"eaf text process\")\n",
        "        \"\"\"\n",
        "        column_data = []\n",
        "        for line in text_source:\n",
        "          temp = line.split('\\n')\n",
        "          \n",
        "          #Extract all of the text info\n",
        "          \n",
        "            print(\"Column names from first row: \")\n",
        "            print(column_names)\n",
        "          else:\n",
        "            tsv_dict[annotation_counter] = {}\n",
        "            tsv_dict[annotation_counter]['REFID'] = filename_stem + \"_\" + str(annotation_counter)\n",
        "            counter = 1\n",
        "            while counter < (total_columns + 1):\n",
        "              try:\n",
        "                tsv_dict[annotation_counter][column_names[(counter - 1)]] = rows[(counter - 1)]\n",
        "              except IndexError:\n",
        "                tsv_dict[annotation_counter][column_names[(counter - 1)]] = \"\"                                  \n",
        "              counter += 1\n",
        "            annotation_counter = annotation_counter + 1\n",
        "            \"\"\"\n",
        "\n",
        "  #If timecode source is being used\n",
        "  if Timecode_Data_Source != None:\n",
        "    #Determine the exact extension\n",
        "    if fn + Timecode_Data_Source in uploaded.keys():\n",
        "      extension = Timecode_Data_Source\n",
        "    if fn + Timecode_Data_Source.upper() in uploaded.keys():\n",
        "      extension = Timecode_Data_Source.upper()\n",
        "    if fn + Timecode_Data_Source.lower() in uploaded.keys():\n",
        "      extension = Timecode_Data_Source.lower()\n",
        "    print(\"Processing timecode from file: \" + fn + extension)\n",
        "\n",
        "    with open(fn + extension) as time_source:\n",
        "      annotation_counter = 1\n",
        "\n",
        "      #.TextGrid timecode extraction process\n",
        "      if Timecode_Data_Source == \".TextGrid\":\n",
        "        print(\".TextGrid timecode process\")   \n",
        "        for line in time_source:\n",
        "          if \"xmin\" in line:\n",
        "            temp = line.split('= ')\n",
        "            temp_xmin = float(temp[1])\n",
        "          if \"xmax\" in line:\n",
        "            temp = line.split('= ')\n",
        "            temp_xmax = float(temp[1])\n",
        "          if Sounding_Interval_Text != \"\":\n",
        "            if \"text = \\\"\" + Sounding_Interval_Text + \"\\\"\" in line:\n",
        "              #Expand timecode data duration\n",
        "              if Expand_Time_Segments == True:\n",
        "                temp_xmin = temp_xmin - (Amount_to_Expand/1000)\n",
        "                temp_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "              try:\n",
        "                tsv_dict[annotation_counter]['XMIN'] = temp_xmin\n",
        "                tsv_dict[annotation_counter]['XMAX'] = temp_xmax\n",
        "                annotation_counter += 1\n",
        "              except KeyError:\n",
        "                print(\"Too many TextGrid segments...\")\n",
        "                print(str(annotation_counter))\n",
        "          else:\n",
        "            if \"text = \\\"\" in line and \"text = \\\"\\\"\" not in line:\n",
        "              #Expand timecode data duration\n",
        "              print(str(annotation_counter))\n",
        "              if Expand_Time_Segments == True:\n",
        "                temp_xmin = temp_xmin - (Amount_to_Expand/1000)\n",
        "                temp_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "              try:\n",
        "                tsv_dict[annotation_counter]['XMIN'] = temp_xmin\n",
        "                tsv_dict[annotation_counter]['XMAX'] = temp_xmax\n",
        "                annotation_counter += 1\n",
        "              except KeyError:\n",
        "                print(\"Too many TextGrid segments...\")\n",
        "                print(str(annotation_counter))\n",
        "\n",
        "          if \"tiers? <exists>\" in line:\n",
        "            if Expand_Time_Segments == True:\n",
        "              final_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "            else:\n",
        "              final_xmax = temp_xmax \n",
        "\n",
        "\n",
        "      #.csv timecode extraction process\n",
        "      if Timecode_Data_Source == \".csv\":\n",
        "        print(\".csv timecode process\")  \n",
        "        CSV_Timecode_Format = 'Seconds' #@param [\"Seconds\", \"Milliseconds\"]\n",
        "        for line in time_source:\n",
        "              temp_xmin = float(line.split(\",\")[0])\n",
        "              temp_xmax = float(line.split(\",\")[1])\n",
        "              if CSV_Timecode_Format == 'Seconds':\n",
        "                temp_xmin = temp_xmin * 1000\n",
        "                temp_xmax = temp_xmax * 1000\n",
        "              if Expand_Time_Segments == True:\n",
        "                temp_xmin = temp_xmin - (Amount_to_Expand/1000)\n",
        "                temp_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "              try:\n",
        "                tsv_dict[annotation_counter]['XMIN'] = temp_xmin\n",
        "                tsv_dict[annotation_counter]['XMAX'] = temp_xmax\n",
        "                annotation_counter += 1\n",
        "              except KeyError:\n",
        "                print(\"Too many timecode segments...\")\n",
        "        if Expand_Time_Segments == True:\n",
        "          final_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "        else:\n",
        "          final_xmax = temp_xmax\n",
        "            \n",
        "      #.eaf timecode extraction process\n",
        "      if Timecode_Data_Source == \".eaf\":\n",
        "        print(\".eaf timecode process\")  \n",
        "        eaf_toggle = False \n",
        "        for line in time_source:\n",
        "          if \"<TIME_SLOT TIME_SLOT_ID=\" in line:\n",
        "            if eaf_toggle == False:\n",
        "              temp_xmin = float(line.split(\"TIME_VALUE=\\\"\")[1].split(\"\\\"/>\")[0])\n",
        "              eaf_toggle = False\n",
        "            if eaf_toggle == True:\n",
        "              temp_xmax = float(line.split(\"TIME_VALUE=\\\"\")[1].split(\"\\\"/>\")[0])\n",
        "              eaf_toggle = False\n",
        "                #Expand timecode data duration\n",
        "              if Expand_Time_Segments == True:\n",
        "                temp_xmin = temp_xmin - (Amount_to_Expand/1000)\n",
        "                temp_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "              try:\n",
        "                tsv_dict[annotation_counter]['XMIN'] = temp_xmin\n",
        "                tsv_dict[annotation_counter]['XMAX'] = temp_xmax\n",
        "                annotation_counter += 1\n",
        "              except KeyError:\n",
        "                print(\"Too many timecode segments...\")\n",
        "        if Expand_Time_Segments == True:\n",
        "          final_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "        else:\n",
        "          final_xmax = temp_xmax\n",
        "        \n",
        "          \n",
        "      #.flextext timecode extraction process\n",
        "      if Timecode_Data_Source == \".flextext\":\n",
        "        print(\".flextext timecode process\")  \n",
        "        flextext_toggle = False \n",
        "        for line in time_source:\n",
        "          if \"begin-time-offset=\" in line:\n",
        "            if flextext_toggle == False:\n",
        "              temp_xmin = float(line.split(\"<phrase begin-time-offset=\\\"\")[1].split(\"\\\"\")[0])\n",
        "              eaf_toggle = False\n",
        "            if eaf_toggle == True:\n",
        "              temp_xmax = float(line.split(\"end-time-offset=\\\"\")[1].split(\"\\\"\")[0])\n",
        "              eaf_toggle = False\n",
        "                #Expand timecode data duration\n",
        "              if Expand_Time_Segments == True:\n",
        "                temp_xmin = temp_xmin - (Amount_to_Expand/1000)\n",
        "                temp_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "              try:\n",
        "                tsv_dict[annotation_counter]['XMIN'] = temp_xmin\n",
        "                tsv_dict[annotation_counter]['XMAX'] = temp_xmax\n",
        "                annotation_counter += 1\n",
        "              except KeyError:\n",
        "                print(\"Too many timecode segments...\")\n",
        "        if Expand_Time_Segments == True:\n",
        "          final_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "        else:\n",
        "          final_xmax = temp_xmax\n",
        "\n",
        "  total_annotations = annotation_counter - 1\n",
        "  #Timecode expansion check\n",
        "  if Expand_Time_Segments == True:\n",
        "    annotation_counter = 2\n",
        "    expansion_check = False\n",
        "    while annotation_counter < (total_annotations + 1):\n",
        "      if tsv_dict[annotation_counter]['XMIN'] < tsv_dict[annotation_counter - 1]['XMAX'] and expansion_check == False:\n",
        "        print(\"Time expansion error with file \" + filename_stem + \". Reverting to original timecode.\")\n",
        "        expansion_check = True\n",
        "        for i in range(total_annotations):\n",
        "          if i > 0:\n",
        "            tsv_dict[i]['XMIN'] = tsv_dict[i]['XMIN'] + (Amount_to_Expand/1000)\n",
        "            tsv_dict[i]['XMAX'] = tsv_dict[i]['XMAX'] - (Amount_to_Expand/1000)\n",
        "      annotation_counter+=1\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "  #Populate new CSV file\n",
        "  with open(new_csv_name, \"w\") as csv_new:\n",
        "    \n",
        "    print(\"Total annotations: \" + str(total_annotations))\n",
        "    annotation_counter = 1\n",
        "    while annotation_counter < (total_annotations + 1):\n",
        "      if Timecode_Data_Source != None:\n",
        "        csv_new.write(tsv_dict[annotation_counter]['REFID'] + ',' + str(tsv_dict[annotation_counter]['XMIN']) + ',' + str(tsv_dict[annotation_counter]['XMAX']))\n",
        "      counter = 1\n",
        "      if Text_Data_Source != None:\n",
        "        while counter < (total_columns + 1):\n",
        "          csv_new.write(',' + tsv_dict[annotation_counter][column_names[(counter - 1)]]) \n",
        "          counter += 1\n",
        "      csv_new.write('\\n')\n",
        "      annotation_counter += 1\n",
        "    \n",
        "         \n",
        "    \n",
        "    #Populate new .flextext file\n",
        "    if Text_Data_Source != \"None\":\n",
        "      with open(new_flextext_name, \"w\") as new_flextext:\n",
        "        document_uuid = str(uuid.uuid4())\n",
        "        media_uuid = str(uuid.uuid4())\n",
        "        analysis_lg_code = \"en\"\n",
        "        document_title = filename_stem\n",
        "\n",
        "        new_flextext.write(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\")\n",
        "        new_flextext.write(\"<document version=\\\"2\\\">\\n\")\n",
        "        new_flextext.write(\"    <interlinear-text guid=\\\"\" + document_uuid +  \"\\\">\\n\")\n",
        "        new_flextext.write(\"        <item lang=\\\"\" + analysis_lg_code + \"\\\" type=\\\"title\\\">\" + document_title + \"</item>\\n\")\n",
        "        new_flextext.write(\"        <paragraphs>\\n\")\n",
        "        \n",
        "\n",
        "        #Cycle through all of the annotations    \n",
        "        annotation_counter = 1\n",
        "        while annotation_counter < (total_annotations + 1):\n",
        "          paragraph_uuid = str(uuid.uuid4())\n",
        "          phrase_uuid = str(uuid.uuid4())\n",
        "          new_flextext.write(\"            <paragraph guid=\\\"\" + paragraph_uuid + \"\\\">\\n\")\n",
        "          new_flextext.write(\"                <phrases>\\n\")\n",
        "          if Timecode_Data_Source != None:\n",
        "            new_flextext.write(\"                    <phrase begin-time-offset=\\\"\" + str(int(tsv_dict[annotation_counter]['XMIN']*1000)) + \"\\\"\\n\")\n",
        "            new_flextext.write(\"                        end-time-offset=\\\"\" + str(int(tsv_dict[annotation_counter]['XMAX']*1000)) + \"\\\"\\n\")\n",
        "          else:\n",
        "            new_flextext.write(\"                    <phrase\\n\")\n",
        "          new_flextext.write(\"                        guid=\\\"\" + phrase_uuid + \"\\\"\\n\")\n",
        "          new_flextext.write(\"                        media-file=\\\"\" + media_uuid + \"\\\" speaker=\\\"A\\\">\\n\")\n",
        "          for i in range(Number_of_Columns):\n",
        "            if column_types[i] == \"Transcription\":\n",
        "              new_flextext.write(\"                        <item lang=\\\"\" + column_lgs[i] + \"\\\" type=\\\"txt\\\">\" + str(tsv_dict[annotation_counter][column_names[i]]) + \"</item>\\n\")\n",
        "          new_flextext.write(\"                        <words/>\\n\")\n",
        "          new_flextext.write(\"                        <item lang=\\\"zxx\\\" type=\\\"gls\\\">\" + tsv_dict[annotation_counter]['REFID'] + \"</item>\\n\")\n",
        "          for i in range(Number_of_Columns):\n",
        "            if column_types[i] == \"Translation\":\n",
        "              new_flextext.write(\"                        <item lang=\\\"\" + column_lgs[i] + \"\\\" type=\\\"gls\\\">\" + str(tsv_dict[annotation_counter][column_names[i]]) + \"</item>\\n\")\n",
        "          new_flextext.write(\"                    </phrase>\\n\")\n",
        "          new_flextext.write(\"                </phrases>\\n\")\n",
        "          new_flextext.write(\"            </paragraph>\\n\")\n",
        "          annotation_counter += 1\n",
        "\n",
        "        new_flextext.write(\"        </paragraphs>\\n\")\n",
        "        new_flextext.write(\"        <media-files offset-type=\\\"\\\">\\n\")\n",
        "        new_flextext.write(\"            <media guid=\\\"\" + media_uuid + \"\\\" location=\\\"\\\"/>\\n\")\n",
        "        new_flextext.write(\"        </media-files>\\n\")\n",
        "        new_flextext.write(\"    </interlinear-text>\\n\")\n",
        "        new_flextext.write(\"</document>\")\n",
        "      new_flextext.close()      \n",
        "\n",
        "    #Populate new .TextGrid file, with three tiers: unique REF ID, transcription, and translation    \n",
        "    print(str(total_annotations))   \n",
        "    if Timecode_Data_Source != \"None\":   \n",
        "      with open(new_textgrid_name, \"w\") as textgrid_new:                          \n",
        "        #Fill out the beginning of the TextGrid file\n",
        "        textgrid_new.write(\"File type = \\\"ooTextFile\\\"\\nObject class = \\\"TextGrid\\\"\\n\\nxmin = 0\\nxmax = \" + str(final_xmax) + \"\\ntiers? <exists>\\nsize = \" + str((total_columns + 1)) + \"\\nitem []:\\n\")\n",
        "        tier_counter = 1\n",
        "        #Fill out the beginning of each tier in the TextGrid\n",
        "        while tier_counter < (total_columns + 2):                                \n",
        "          textgrid_new.write(\"    item [\" + str(tier_counter) + \"]:\\n\")\n",
        "          textgrid_new.write(\"        class = \\\"IntervalTier\\\"\\n\")\n",
        "          if tier_counter == 1:\n",
        "            textgrid_new.write(\"        name = \\\"REFID\\\"\\n\")\n",
        "          else:\n",
        "            if Text_Data_Source != \"None\":\n",
        "              textgrid_new.write(\"        name = \\\"\" + column_names[(tier_counter - 2)] + \"\\\"\\n\")               \n",
        "          textgrid_new.write(\"        xmin = 0\\n\")\n",
        "          textgrid_new.write(\"        xmax = \" + str(final_xmax) + \"\\n\")\n",
        "          textgrid_new.write(\"        intervals: size = \" + str((total_annotations * 2) + 1) + \"\\n\")\n",
        "          #Fill out each TextGrid interval using the dictionary\n",
        "          annotation_counter = 1\n",
        "          while annotation_counter < (total_annotations + 1):\n",
        "            if annotation_counter == 1:\n",
        "              temp_prev_xmax = 0\n",
        "            else:\n",
        "              temp_prev_xmax = tsv_dict[annotation_counter - 1]['XMAX']\n",
        "            temp_next_xmin = tsv_dict[annotation_counter]['XMIN']\n",
        "            textgrid_new.write(\"        intervals [\" + str(((annotation_counter * 2) - 1)) + \"]:\\n\")\n",
        "            if annotation_counter == 1:\n",
        "              textgrid_new.write(\"            xmin = 0\\n\")\n",
        "            else:\n",
        "              textgrid_new.write(\"            xmin = \" + str(temp_prev_xmax) + \"\\n\")\n",
        "            textgrid_new.write(\"            xmax = \" + str(temp_next_xmin) + \"\\n\")\n",
        "            textgrid_new.write(\"            text = \\\"\\\"\\n\")\n",
        "            textgrid_new.write(\"        intervals [\" + str(annotation_counter * 2) + \"]:\\n\")\n",
        "            textgrid_new.write(\"            xmin = \" + str(tsv_dict[annotation_counter]['XMIN']) + \"\\n\")\n",
        "            textgrid_new.write(\"            xmax = \" + str(tsv_dict[annotation_counter]['XMAX']) + \"\\n\")\n",
        "            if tier_counter == 1:\n",
        "              textgrid_new.write(\"            text = \\\"\" + str(tsv_dict[annotation_counter]['REFID']) + \"\\\"\\n\")\n",
        "            else:\n",
        "              if Text_Data_Source != \"None\":\n",
        "                textgrid_new.write(\"            text = \\\"\" + str(tsv_dict[annotation_counter][column_names[(tier_counter - 2)]]) + \"\\\"\\n\")\n",
        "              else:\n",
        "                textgrid_new.write(\"            text = \\\"\\\"\\n\")\n",
        "            annotation_counter += 1\n",
        "          textgrid_new.write(\"        intervals [\" + str((((annotation_counter - 1) * 2) + 1)) + \"]:\\n\")\n",
        "          xmax_of_final_annotation = tsv_dict[total_annotations]['XMAX']\n",
        "          textgrid_new.write(\"            xmin = \" + str(xmax_of_final_annotation) + \"\\n\")\n",
        "          textgrid_new.write(\"            xmax = \" + str(final_xmax) + \"\\n\")\n",
        "          textgrid_new.write(\"            text = \\\"\\\"\\n\")\n",
        "          tier_counter += 1\n",
        "      textgrid_new.close()                                 \n",
        "\n",
        "#Populate new .EAF file\n",
        "#Fill out the beginning of the EAF file\n",
        "    if Timecode_Data_Source != \"None\":\n",
        "      with open(new_eaf_name, \"w\") as eaf_new:\n",
        "        eaf_new.write(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\")\n",
        "        eaf_new.write(\"<ANNOTATION_DOCUMENT AUTHOR=\\\"unspecified\\\" DATE=\\\"\" + str(now.year) + \"-\" + str(now.month) + \"-\" + str(now.day) + \"T\" + str(now.hour) + \":\" + str(now.minute) + \":\" + str(now.second) + \"-08:00\\\" FORMAT=\\\"3.0\\\" VERSION=\\\"3.0\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:noNamespaceSchemaLocation=\\\"http://www.mpi.nl/tools/elan/EAFv3.0.xsd\\\">\\n\")\n",
        "        eaf_new.write(\"    <HEADER MEDIA_FILE=\\\"\\\" TIME_UNITS=\\\"milliseconds\\\">\\n\")\n",
        "  #Fixes slashes in Windows directories\n",
        "        if Operating_System == 'Windows':                   \n",
        "          eaf_new.write(\"        <MEDIA_DESCRIPTOR MEDIA_URL=\\\"file:///\" + filename_stem + \".wav\\\" MIME_TYPE=\\\"audio/x-wav\\\" RELATIVE_MEDIA_URL=\\\"./\" + filename_stem + \".wav\\\"/>\\n\")\n",
        "        else:\n",
        "          eaf_new.write(\"        <MEDIA_DESCRIPTOR MEDIA_URL=\\\"file://\" + filename_stem + \".wav\\\" MIME_TYPE=\\\"audio/x-wav\\\" RELATIVE_MEDIA_URL=\\\"./\" + filename_stem + \".wav\\\"/>\\n\")\n",
        "        eaf_new.write(\"        <PROPERTY NAME=\\\"URN\\\">urn:nl-mpi-tools-elan-eaf:93cd58ea-4af9-44d5-a6d5-d468217ccf5e</PROPERTY>\\n\")\n",
        "        eaf_new.write(\"        <PROPERTY NAME=\\\"lastUsedAnnotationId\\\">\" + str((5 * total_annotations)) + \"</PROPERTY>\\n\")\n",
        "        eaf_new.write(\"    </HEADER>\\n\")\n",
        "        eaf_new.write(\"    <TIME_ORDER>\\n\")                       \n",
        "  #Fill out time slots\n",
        "        annotation_counter = 1\n",
        "        while annotation_counter < (total_annotations + 1):\n",
        "          eaf_new.write(\"        <TIME_SLOT TIME_SLOT_ID=\\\"ts\" + str((annotation_counter * 2) - 1) + \"\\\" TIME_VALUE=\\\"\" + str(int(1000 * tsv_dict[annotation_counter]['XMIN'])) + \"\\\"/>\\n\")\n",
        "          eaf_new.write(\"        <TIME_SLOT TIME_SLOT_ID=\\\"ts\" + str((annotation_counter * 2)) + \"\\\" TIME_VALUE=\\\"\" + str(int(1000 * tsv_dict[annotation_counter]['XMAX'])) + \"\\\"/>\\n\")\n",
        "          annotation_counter += 1\n",
        "  #Fill out RFID annotations\n",
        "        eaf_new.write(\"    </TIME_ORDER>\\n\")\n",
        "        eaf_new.write(\"    <TIER DEFAULT_LOCALE=\\\"en\\\" LINGUISTIC_TYPE_REF=\\\"REFID\\\" TIER_ID=\\\"REFID\\\">\\n\")\n",
        "        annotation_counter = 1\n",
        "        while annotation_counter < (total_annotations + 1):\n",
        "          eaf_new.write(\"        <ANNOTATION>\\n\")\n",
        "          eaf_new.write(\"            <ALIGNABLE_ANNOTATION ANNOTATION_ID=\\\"a\" + str((2 * total_annotations) + annotation_counter) + \"\\\" TIME_SLOT_REF1=\\\"ts\" + str((annotation_counter * 2) -1) + \"\\\" TIME_SLOT_REF2=\\\"ts\" + str(annotation_counter * 2) + \"\\\">\\n\")\n",
        "          eaf_new.write(\"                <ANNOTATION_VALUE>\" + str(tsv_dict[annotation_counter]['REFID']) + \"</ANNOTATION_VALUE>\\n\")\n",
        "          eaf_new.write(\"            </ALIGNABLE_ANNOTATION>\\n\")\n",
        "          eaf_new.write(\"        </ANNOTATION>\\n\")\n",
        "          annotation_counter += 1\n",
        "        eaf_new.write(\"    </TIER>\\n\")                \n",
        "  #Fill out transcription and translation annotations\n",
        "        if Text_Data_Source != \"None\":\n",
        "          tier_counter = 1\n",
        "          while tier_counter < total_columns + 1:                        \n",
        "            eaf_new.write(\"    <TIER DEFAULT_LOCALE=\\\"en\\\" LINGUISTIC_TYPE_REF=\\\"\" + column_names[(tier_counter - 1)] + \"\\\" PARENT_REF=\\\"REFID\\\" TIER_ID=\\\"\" + column_names[(tier_counter - 1)] + \"\\\">\\n\")\n",
        "            annotation_counter = 1\n",
        "            while annotation_counter < (total_annotations + 1):\n",
        "              eaf_new.write(\"        <ANNOTATION>\\n\")\n",
        "              eaf_new.write(\"            <REF_ANNOTATION ANNOTATION_ID=\\\"a\" + str(((tier_counter + 2) * total_annotations) + annotation_counter) + \"\\\" ANNOTATION_REF=\\\"a\" + str((2 * total_annotations) + annotation_counter) + \"\\\">\\n\")\n",
        "              eaf_new.write(\"                <ANNOTATION_VALUE>\" + str(tsv_dict[annotation_counter][column_names[(tier_counter - 1)]]) + \"</ANNOTATION_VALUE>\\n\")\n",
        "              eaf_new.write(\"            </REF_ANNOTATION>\\n\")\n",
        "              eaf_new.write(\"        </ANNOTATION>\\n\")\n",
        "              annotation_counter += 1\n",
        "            eaf_new.write(\"    </TIER>\\n\")\n",
        "            tier_counter += 1\n",
        "                      \n",
        "                                \n",
        "  #Fill out end of the EAF file\n",
        "        eaf_new.write(\"    <LINGUISTIC_TYPE GRAPHIC_REFERENCES=\\\"false\\\" LINGUISTIC_TYPE_ID=\\\"REFID\\\" TIME_ALIGNABLE=\\\"true\\\"/>\\n\")\n",
        "        if Text_Data_Source != \"None\":\n",
        "          counter = 1\n",
        "          while counter < (total_columns + 1):\n",
        "            eaf_new.write(\"    <LINGUISTIC_TYPE CONSTRAINTS=\\\"Symbolic_Association\\\" GRAPHIC_REFERENCES=\\\"false\\\" LINGUISTIC_TYPE_ID=\\\"\" + column_names[(counter - 1)] + \"\\\" TIME_ALIGNABLE=\\\"false\\\"/>\\n\")    \n",
        "            counter += 1\n",
        "        eaf_new.write(\"    <LOCALE COUNTRY_CODE=\\\"US\\\" LANGUAGE_CODE=\\\"en\\\"/>\\n\")\n",
        "        eaf_new.write(\"    <CONSTRAINT DESCRIPTION=\\\"Time subdivision of parent annotation's time interval, no time gaps allowed within this interval\\\" STEREOTYPE=\\\"Time_Subdivision\\\"/>\\n\")\n",
        "        eaf_new.write(\"    <CONSTRAINT DESCRIPTION=\\\"Symbolic subdivision of a parent annotation. Annotations refering to the same parent are ordered\\\" STEREOTYPE=\\\"Symbolic_Subdivision\\\"/>\\n\")\n",
        "        eaf_new.write(\"    <CONSTRAINT DESCRIPTION=\\\"1-1 association with a parent annotation\\\" STEREOTYPE=\\\"Symbolic_Association\\\"/>\\n\")\n",
        "        eaf_new.write(\"    <CONSTRAINT DESCRIPTION=\\\"Time alignable annotations within the parent annotation's time interval, gaps are allowed\\\" STEREOTYPE=\\\"Included_In\\\"/>\\n\")\n",
        "        eaf_new.write(\"</ANNOTATION_DOCUMENT>\")\n",
        "      \n",
        "#      print('REFID\\t' + 'XMIN\\t' + 'XMAX', end='')\n",
        "#      for x in column_names:\n",
        "#        print('\\t' + x, end='')\n",
        "#      print('')\n",
        "#      annotation_counter = 1\n",
        "#      while annotation_counter < (total_annotations + 1):\n",
        "#        print(tsv_dict[annotation_counter]['REFID'] + '\\t' + str(tsv_dict[annotation_counter]['XMIN']) + '\\t' + str(tsv_dict[annotation_counter]['XMAX']), end='')\n",
        "#        for y in column_names:\n",
        "#          print('\\t' + tsv_dict[annotation_counter][y], end='')\n",
        "#        annotation_counter+=1\n",
        "#        print('')    \n",
        "    print(\"Processing complete for: \" + fn)\n",
        "!zip -r Output/Zippped_Files.zip Output/\n",
        "files.download('Output/Zippped_Files.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkXXCTWHLNO0"
      },
      "source": [
        ""
      ]
    }
  ]
}