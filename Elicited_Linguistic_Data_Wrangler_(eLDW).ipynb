{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Elicited Linguistic Data Wrangler (eLDW).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4pfwKcwLKhs"
      },
      "source": [
        "#eLDW\n",
        "Description: This script combines and reformats elicited linguistic data. See the [GitHub page](https://github.com/rgriscom/Elicited-Linguistic-Data-Wrangler) for more information.\n",
        "\n",
        "**How to use the script**\n",
        "1. Choose the options in the first form and press the first play button.\n",
        "2. Fill out the information in the generated form and press the second play button.\n",
        "3. Press the \"Browse\" button to upload your files.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNbOn2pELLIw",
        "cellView": "form"
      },
      "source": [
        "import uuid, ipywidgets as widgets, pandas as pd\n",
        "from IPython.display import display\n",
        "from __future__ import print_function\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "\n",
        "#@title Settings\n",
        "Operating_System = 'Linux' #@param [\"Linux\", \"MacOS\", \"Windows\"]\n",
        "Text_Data_Source = '.csv' #@param [\".csv\", \".TextGrid\", \".eaf\", \".flextext\", \"None\"]\n",
        "Timecode_Data_Source = '.TextGrid' #@param [\".csv\", \".TextGrid\", \".eaf\", \".flextext\",\"None\"]\n",
        "Add_REFID_Tier = True #@param {type:\"boolean\"}\n",
        "\n",
        "Sounding_Interval_Text = ''\n",
        "CSV_Timecode_Format = 'Seconds'\n",
        "Expand_Time_Segments = False\n",
        "Amount_to_Expand = 10\n",
        "\n",
        "### Text data settings\n",
        "if Text_Data_Source == '.flextext':\n",
        "  column_names = []\n",
        "  column_types = []\n",
        "  column_lgs = []\n",
        "if Text_Data_Source == '.csv' or Text_Data_Source == '.eaf' or Text_Data_Source == '.TextGrid':\n",
        "  column_names = []\n",
        "  column_types = []\n",
        "  column_lgs = []\n",
        "  number_of_columns = 3\n",
        "  column_visibility = [1,1,1,0,0,0,0,0,0,0]\n",
        "  out = widgets.Output()\n",
        "  display(out)\n",
        "  CSV_Delimiter = \"Comma\"\n",
        "  column_name_entry_list = []\n",
        "  column_type_entry_list = []\n",
        "  column_lg_entry_list = []\n",
        "  First_Row_Is_Column_Names = False\n",
        "  j = 0\n",
        "  @out.capture()\n",
        "  def name_handle_submit(change):\n",
        "    try:\n",
        "      column_names[int(change['owner'].description.split(' name:')[0])-1] = change['new']\n",
        "    except:\n",
        "      column_names.append(change['new'])\n",
        "\n",
        "  @out.capture()\n",
        "  def type_handle_submit(change):\n",
        "    try:\n",
        "      column_types[int(change['owner'].description.split(' type:')[0])-1] = change['new']\n",
        "    except:\n",
        "      column_types.append(change['new'])\n",
        "\n",
        "  @out.capture()\n",
        "  def lg_handle_submit(change):\n",
        "    try:\n",
        "      column_lgs[int(change['owner'].description.split(' ISO 639:')[0])-1] = change['new']\n",
        "    except:\n",
        "      column_lgs.append(change['new'])\n",
        "\n",
        "  @out.capture()\n",
        "  def slider_handle_submit(change):  \n",
        "    global number_of_columns\n",
        "    global First_Row_Is_Column_Names \n",
        "    number_of_columns = change['new']\n",
        "    for i in range(10):\n",
        "      if (i+1) <= number_of_columns:\n",
        "        if column_type_entry_list[i].layout.display == 'none':\n",
        "          column_type_entry_list[i].layout.display = 'flex'\n",
        "        if column_lg_entry_list[i].layout.display == 'none':\n",
        "          column_lg_entry_list[i].layout.display = 'flex'\n",
        "        if column_name_entry_list[i].layout.display == 'none' and First_Row_Is_Column_Names == False:\n",
        "          column_name_entry_list[i].layout.display = 'flex'\n",
        "      if (i+1) > number_of_columns:\n",
        "        if column_type_entry_list[i].layout.display == 'flex':\n",
        "          column_type_entry_list[i].layout.display = 'none'\n",
        "        if column_lg_entry_list[i].layout.display == 'flex':\n",
        "          column_lg_entry_list[i].layout.display = 'none'\n",
        "        if column_name_entry_list[i].layout.display == 'flex':\n",
        "          column_name_entry_list[i].layout.display = 'none'\n",
        "  \n",
        "\n",
        "  column_slider = widgets.IntSlider(\n",
        "      value=3,\n",
        "      min=1,\n",
        "      max=10,\n",
        "      step=1,\n",
        "      description='Column_slider',\n",
        "      disabled=False,\n",
        "      continuous_update=True,\n",
        "      orientation='horizontal',\n",
        "      readout=True,\n",
        "      readout_format='d',\n",
        "      style=dict(description_width='initial')\n",
        "  )\n",
        "  display(column_slider)  \n",
        "  \n",
        "  for i in range(10):\n",
        "    if First_Row_Is_Column_Names == False:\n",
        "      column_name_entry_list.append(widgets.Text(\n",
        "        value='',\n",
        "        placeholder='Name',\n",
        "        description=str(i+1) + \" name:\",\n",
        "        continuous_update=True,\n",
        "        disabled=False,\n",
        "        style=dict(description_width='initial')))\n",
        "      display(column_name_entry_list[i])\n",
        "      if (i+1) > number_of_columns:\n",
        "        column_name_entry_list[i].layout.display = 'none'\n",
        "      else:\n",
        "        column_name_entry_list[i].layout.display = 'flex'\n",
        "    column_type_entry_list.append(widgets.Dropdown(\n",
        "      options=['Select One','Transcription', 'Translation', 'Notes'],\n",
        "      value='Select One',\n",
        "      description=str(i+1) + \" type:\",\n",
        "      disabled=False,\n",
        "      style=dict(description_width='initial')))\n",
        "    display(column_type_entry_list[i])\n",
        "    if (i+1) > number_of_columns:\n",
        "      column_type_entry_list[i].layout.display = 'none'\n",
        "    else:\n",
        "      column_type_entry_list[i].layout.display = 'flex'\n",
        "    column_lg_entry_list.append(widgets.Text(\n",
        "      value='',\n",
        "      placeholder='ISO 639 language code',\n",
        "      description=str(i+1) + \" ISO 639:\",\n",
        "      continuous_update=True,\n",
        "      disabled=False,\n",
        "      style=dict(description_width='initial')))\n",
        "    display(column_lg_entry_list[i])\n",
        "    if (i+1) > number_of_columns:\n",
        "      column_lg_entry_list[i].layout.display = 'none'\n",
        "    else:\n",
        "      column_lg_entry_list[i].layout.display = 'flex'\n",
        "\n",
        "  column_slider.observe(slider_handle_submit, names='value')\n",
        "\n",
        "  for i in range(number_of_columns):\n",
        "    if First_Row_Is_Column_Names == False:\n",
        "      column_name_entry_list[i].observe(name_handle_submit, names='value')\n",
        "    column_type_entry_list[i].observe(type_handle_submit, names='value')\n",
        "    column_lg_entry_list[i].observe(lg_handle_submit, names='value')\n",
        "\n",
        "if Text_Data_Source == '.csv':\n",
        "  @out.capture()\n",
        "  def delimiter_handle_submit(change):\n",
        "    global CSV_Delimiter\n",
        "    CSV_Delimiter = change['new']\n",
        "  \n",
        "  @out.capture()\n",
        "  def first_row_handle_submit(change):\n",
        "    global First_Row_Is_Column_Names\n",
        "    First_Row_Is_Column_Names = change['new']\n",
        "    for i in range(10):\n",
        "      if First_Row_Is_Column_Names == True:\n",
        "          column_name_entry_list[i].layout.display = 'none'\n",
        "      if First_Row_Is_Column_Names == False and (i+1) <= number_of_columns:\n",
        "          column_name_entry_list[i].layout.display = 'flex'\n",
        "  csv_delimiter_dropdown = widgets.Dropdown(\n",
        "      options=['Comma','Tab','Semi-colon'],\n",
        "      value='Comma',\n",
        "      description=\"CSV Delimiter:\",\n",
        "      disabled=False,\n",
        "      style=dict(description_width='initial'),\n",
        "      layout=dict(width='400px')\n",
        "      )\n",
        "  display(csv_delimiter_dropdown)\n",
        "  First_Row_Is_Column_Names_checkbox = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='First row of CSV is column titles?',\n",
        "    disabled=False,\n",
        "    indent=False,\n",
        "    style=dict(description_width='initial'),\n",
        "    layout=dict(width='400px')\n",
        "  )\n",
        "  display(First_Row_Is_Column_Names_checkbox)\n",
        "  csv_delimiter_dropdown.observe(delimiter_handle_submit, names='value')\n",
        "  First_Row_Is_Column_Names_checkbox.observe(first_row_handle_submit, names='value')\n",
        "  \n",
        "###Time code settings\n",
        "if Timecode_Data_Source != 'None':\n",
        "  @out.capture()\n",
        "  def time_segments_checkbox_handle_submit(change):\n",
        "    global Expand_Time_Segments\n",
        "    Expand_Time_Segments = change['new']\n",
        "    if Expand_Time_Segments == True:\n",
        "      Amount_to_Expand_slider.layout.display = 'flex'\n",
        "    if Expand_Time_Segments == False:\n",
        "      Amount_to_Expand_slider.layout.display = 'none'\n",
        "\n",
        "  @out.capture()\n",
        "  def amount_to_expand_slider_handle_submit(change):\n",
        "    global Amount_to_Expand\n",
        "    Amount_to_Expand = change['new']\n",
        "\n",
        "  Expand_Time_Segments_checkbox = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Expand duration of time segments',\n",
        "    disabled=False,\n",
        "    indent=False,\n",
        "    style=dict(description_width='initial')\n",
        "  )\n",
        "  display(Expand_Time_Segments_checkbox)\n",
        "  Amount_to_Expand_slider = widgets.IntSlider(\n",
        "      value=10,\n",
        "      min=0,\n",
        "      max=100,\n",
        "      step=10,\n",
        "      description='Amount',\n",
        "      disabled=False,\n",
        "      continuous_update=True,\n",
        "      orientation='horizontal',\n",
        "      readout=True,\n",
        "      readout_format='d',\n",
        "  )\n",
        "  display(Amount_to_Expand_slider)\n",
        "  Amount_to_Expand_slider.layout.display = 'none'\n",
        "  Amount_to_Expand_slider.observe(amount_to_expand_slider_handle_submit, names='value')\n",
        "  Expand_Time_Segments_checkbox.observe(time_segments_checkbox_handle_submit, names='value')\n",
        "\n",
        "if Timecode_Data_Source == '.TextGrid':\n",
        "  @out.capture()\n",
        "  def sound_interval_submit(change):\n",
        "    global Sounding_Interval_Text\n",
        "    Sounding_Interval_Text = change['new']\n",
        "\n",
        "  Sounding_Interval_textbox = widgets.Text(\n",
        "      value='',\n",
        "      placeholder='Sounding Interval in TextGrid file',\n",
        "      description=\"Sounding Interval\",\n",
        "      continuous_update=True,\n",
        "      disabled=False,\n",
        "      style=dict(description_width='initial'),\n",
        "      layout = widgets.Layout(width='50%')\n",
        "  )\n",
        "  display(Sounding_Interval_textbox)\n",
        "\n",
        "  Sounding_Interval_textbox.observe(sound_interval_submit, names='value')\n",
        "\n",
        "if Timecode_Data_Source == '.csv':\n",
        "  @out.capture()\n",
        "  def csv_time_submit(change):\n",
        "    global CSV_Timecode_Format\n",
        "    CSV_Timecode_Format = change['new']\n",
        "\n",
        "  CSV_Timecode_Format_dropdown = widgets.Dropdown(\n",
        "      options=['Seconds','Milliseconds'],\n",
        "      value='Seconds',\n",
        "      description=\"CSV Time Format: \",\n",
        "      disabled=False,\n",
        "      style=dict(description_width='initial')\n",
        "  )\n",
        "  display(CSV_Timecode_Format_dropdown)\n",
        "\n",
        "  CSV_Timecode_Format_dropdown.observe(csv_time_submit, names='value')\n",
        "\n",
        "\n",
        "   \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fumRor_RYHl",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "try:\n",
        "  !rm -rf Output\n",
        "  !mkdir Output\n",
        "except:\n",
        "  !mkdir Output\n",
        "from google.colab import files\n",
        "import datetime\n",
        "now = datetime.datetime.now()\n",
        "uploaded = files.upload()     \n",
        "wrong_file_format = 0\n",
        "\n",
        "if First_Row_Is_Column_Names == True or Text_Data_Source == \".flextext\" or Text_Data_Source == \"None\":\n",
        "  column_names = []\n",
        "\n",
        "\n",
        "list_of_files = []\n",
        "for fn in uploaded.keys():\n",
        "  filename_stem = fn.split(\".\")[0]\n",
        "  if filename_stem not in list_of_files:\n",
        "    list_of_files.append(filename_stem)\n",
        "\n",
        "#For each file in the list of files\n",
        "for fn in list_of_files:\n",
        "  new_csv_name = \"Output/\" + fn + \".csv\"\n",
        "  new_textgrid_name = \"Output/\" + fn + \".TextGrid\"\n",
        "  new_flextext_name = \"Output/\" + fn + \".flextext\"\n",
        "  new_eaf_name = \"Output/\" + fn + \".eaf\"\n",
        "  columns_list = column_names.copy()\n",
        "  columns_list.extend(['REFID','XMIN','XMAX'])\n",
        "  tsv_df = pd.DataFrame(columns=columns_list)  \n",
        "\n",
        "#INPUT PROCESSES\n",
        "  #If text source is being used\n",
        "  if Text_Data_Source != None:\n",
        "    #Determine the exact extension\n",
        "    if fn + Text_Data_Source in uploaded.keys():\n",
        "      extension = Text_Data_Source\n",
        "    if fn + Text_Data_Source.upper() in uploaded.keys():\n",
        "      extension = Text_Data_Source.upper()\n",
        "    if fn + Text_Data_Source.lower() in uploaded.keys():\n",
        "      extension = Text_Data_Source.lower()\n",
        "    print(\"Processing text from file: \" + fn + extension)\n",
        "    \n",
        "    with open(fn + extension) as text_source:\n",
        "      annotation_counter = 1\n",
        "      \n",
        "      #.CSV text extraction process\n",
        "      if Text_Data_Source == \".csv\":\n",
        "        if CSV_Delimiter == \"Comma\":\n",
        "          delim = \",\"\n",
        "        if CSV_Delimiter == \"Tab\":\n",
        "          delim = \"\\t\"\n",
        "        if CSV_Delimiter == \"Semi-colon\":\n",
        "          delim = \";\"\n",
        "        names_check = False\n",
        "        column_data = []\n",
        "        for line in text_source:\n",
        "          temp = line.split('\\n')\n",
        "          rows = temp[0].split(delim)\n",
        "          if First_Row_Is_Column_Names == True and names_check == False:\n",
        "            for i in rows:\n",
        "              column_names.append(i)\n",
        "            total_columns = len(column_names)\n",
        "            print(\"Column names from first row: \")\n",
        "            print(column_names)\n",
        "            names_check = True\n",
        "          else:\n",
        "            counter = 1\n",
        "            while counter < (total_columns + 1):\n",
        "              try:\n",
        "                \n",
        "                tsv_df.at[annotation_counter, column_names[(counter - 1)]] = rows[(counter - 1)]\n",
        "              except IndexError:\n",
        "                \n",
        "                tsv_df.at[annotation_counter, column_names[(counter - 1)]] = \"\"                                  \n",
        "              counter += 1\n",
        "            annotation_counter = annotation_counter + 1\n",
        "       \n",
        "\n",
        "      #.TextGrid text extraction process\n",
        "      if Text_Data_Source == \".TextGrid\":\n",
        "        tier_counter = 0\n",
        "        annotation_counter = 1\n",
        "        for line in text_source:\n",
        "          if \"name = \\\"\" in line:\n",
        "            tier_name = line.split('name = \\\"')[1].split(\"\\\"\")[0]\n",
        "            if tier_name in column_names:\n",
        "              tier_counter += 1\n",
        "              annotation_counter = 1\n",
        "             \n",
        "          if \"text = \\\"\" in line and \"text = \\\"\\\"\" not in line:\n",
        "            annotation = line.split('text = \\\"')[1].split(\"\\\"\")[0]\n",
        "            tsv_df.at[annotation_counter, tier_name] = annotation\n",
        "            annotation_counter += 1\n",
        "         \n",
        "\n",
        "        \n",
        "    \n",
        "\n",
        "      #.flextext text extraction process\n",
        "      if Text_Data_Source == \".flextext\": \n",
        "        tier_counter = 0\n",
        "        annotation_counter = 0\n",
        "        for line in text_source:\n",
        "          if 'type=\\\"txt\\\"' in line:\n",
        "            tier_language = line.split('lang=\\\"')[1].split('\\\"')[0]\n",
        "            tier_name = 'A_Transcription-txt-' + tier_language \n",
        "            if tier_name not in column_names:\n",
        "              column_names.append(tier_name)\n",
        "              column_lgs.append(tier_language)\n",
        "              column_types.append('Transcription')\n",
        "              tier_counter += 1\n",
        "            annotation = line.split('type=\\\"txt\\\">')[1].split(\"</item>\")[0]\n",
        "            annotation_counter += 1\n",
        "            tsv_df.at[annotation_counter, tier_name] = annotation\n",
        "          if 'type=\\\"gls\\\"' in line: \n",
        "            tier_language = line.split('lang=\\\"')[1].split('\\\"')[0]\n",
        "            tier_name = 'A_Translation-gls-' + tier_language           \n",
        "            if tier_name not in column_names:\n",
        "              column_names.append(tier_name)\n",
        "              column_lgs.append(tier_language)\n",
        "              column_types.append('Translation')\n",
        "              tier_counter += 1\n",
        "            annotation = line.split('type=\\\"gls\\\">')[1].split(\"</item>\")[0]\n",
        "            \n",
        "            tsv_df.at[annotation_counter, tier_name] = annotation\n",
        "        number_of_columns = len(column_names)\n",
        "        \n",
        "            \n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "      #.eaf text extraction process\n",
        "      if Text_Data_Source == \".eaf\":\n",
        "        tier_counter = 0\n",
        "        annotation_counter = 1\n",
        "        for line in text_source:\n",
        "          if \"TIER_ID=\" in line and \"/>\" not in line:\n",
        "            tier_name = line.split('TIER_ID=\\\"')[1].split(\"\\\">\")[0]\n",
        "            if tier_name in column_names:\n",
        "              tier_counter += 1\n",
        "              annotation_counter = 1\n",
        "          if \"<ANNOTATION_VALUE>\" in line: \n",
        "            annotation = line.split('<ANNOTATION_VALUE>')[1].split(\"</ANNOTATION_VALUE>\")[0]\n",
        "            tsv_df.at[annotation_counter, tier_name] = annotation\n",
        "            annotation_counter += 1\n",
        "        \n",
        "\n",
        "  #If timecode source is being used\n",
        "  if Timecode_Data_Source != None:\n",
        "    #Determine the exact extension\n",
        "    if fn + Timecode_Data_Source in uploaded.keys():\n",
        "      extension = Timecode_Data_Source\n",
        "    if fn + Timecode_Data_Source.upper() in uploaded.keys():\n",
        "      extension = Timecode_Data_Source.upper()\n",
        "    if fn + Timecode_Data_Source.lower() in uploaded.keys():\n",
        "      extension = Timecode_Data_Source.lower()\n",
        "    print(\"Processing timecode from file: \" + fn + extension)\n",
        "\n",
        "    with open(fn + extension) as time_source:\n",
        "      annotation_counter = 1\n",
        "\n",
        "      #.TextGrid timecode extraction process\n",
        "      if Timecode_Data_Source == \".TextGrid\":  \n",
        "        for line in time_source:\n",
        "          if \"xmin\" in line:\n",
        "            temp = line.split('= ')\n",
        "            temp_xmin = float(temp[1])\n",
        "          if \"xmax\" in line:\n",
        "            temp = line.split('= ')\n",
        "            temp_xmax = float(temp[1])\n",
        "          if \"name = \\\"\" in line:\n",
        "            annotation_counter = 1\n",
        "          if Sounding_Interval_Text != \"\":\n",
        "            if \"text = \\\"\" + Sounding_Interval_Text + \"\\\"\" in line:\n",
        "              #Expand timecode data duration\n",
        "              if Expand_Time_Segments == True:\n",
        "                temp_xmin = temp_xmin - (Amount_to_Expand/1000)\n",
        "                temp_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "              try:\n",
        "                tsv_df.at[annotation_counter, 'XMIN'] = temp_xmin\n",
        "                tsv_df.at[annotation_counter, 'XMAX'] = temp_xmax\n",
        "                annotation_counter += 1\n",
        "              except KeyError:\n",
        "                print(\"Too many TextGrid segments...\")\n",
        "                print(str(annotation_counter))\n",
        "          else:\n",
        "            if \"text = \\\"\" in line and \"text = \\\"\\\"\" not in line:\n",
        "              #Expand timecode data duration\n",
        "              \n",
        "              if Expand_Time_Segments == True:\n",
        "                temp_xmin = temp_xmin - (Amount_to_Expand/1000)\n",
        "                temp_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "              try:\n",
        "                tsv_df.at[annotation_counter, 'XMIN'] = temp_xmin\n",
        "                tsv_df.at[annotation_counter, 'XMAX'] = temp_xmax\n",
        "                annotation_counter += 1\n",
        "              except KeyError:\n",
        "                print(\"Too many TextGrid segments...\")\n",
        "                print(str(annotation_counter))\n",
        "\n",
        "          if \"tiers? <exists>\" in line:\n",
        "            if Expand_Time_Segments == True:\n",
        "              final_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "            else:\n",
        "              final_xmax = temp_xmax \n",
        "\n",
        "\n",
        "      #.csv timecode extraction process\n",
        "      if Timecode_Data_Source == \".csv\":\n",
        "        #CSV_Timecode_Format = 'Seconds' #@param [\"Seconds\", \"Milliseconds\"]\n",
        "        for line in time_source:\n",
        "              temp_xmin = float(line.split(\",\")[0])\n",
        "              temp_xmax = float(line.split(\",\")[1])\n",
        "              if CSV_Timecode_Format == 'Seconds':\n",
        "                temp_xmin = temp_xmin * 1000\n",
        "                temp_xmax = temp_xmax * 1000\n",
        "              if Expand_Time_Segments == True:\n",
        "                temp_xmin = temp_xmin - (Amount_to_Expand/1000)\n",
        "                temp_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "              try:\n",
        "                tsv_df.at[annotation_counter, 'XMIN'] = temp_xmin\n",
        "                tsv_df.at[annotation_counter, 'XMAX'] = temp_xmax\n",
        "                annotation_counter += 1\n",
        "              except KeyError:\n",
        "                print(\"Too many timecode segments...\")\n",
        "        if Expand_Time_Segments == True:\n",
        "          final_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "        else:\n",
        "          final_xmax = (temp_xmax/1000)\n",
        "            \n",
        "      #.eaf timecode extraction process\n",
        "      if Timecode_Data_Source == \".eaf\":\n",
        "        eaf_toggle = False \n",
        "        for line in time_source:\n",
        "          if \"<TIME_SLOT TIME_SLOT_ID=\" in line:\n",
        "            if eaf_toggle == True:\n",
        "              temp_xmax = float(line.split(\"TIME_VALUE=\\\"\")[1].split(\"\\\"/>\")[0])\n",
        "              eaf_toggle = False\n",
        "                #Expand timecode data duration\n",
        "              if Expand_Time_Segments == True:\n",
        "                temp_xmin = temp_xmin - (Amount_to_Expand/1000)\n",
        "                temp_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "              try:\n",
        "                tsv_df.at[annotation_counter, 'XMIN'] = temp_xmin/1000\n",
        "                tsv_df.at[annotation_counter, 'XMAX'] = temp_xmax/1000\n",
        "                annotation_counter += 1\n",
        "              except KeyError:\n",
        "                print(\"Too many timecode segments...\")\n",
        "            else:\n",
        "              temp_xmin = float(line.split(\"TIME_VALUE=\\\"\")[1].split(\"\\\"/>\")[0])\n",
        "              eaf_toggle = True\n",
        "        if Expand_Time_Segments == True:\n",
        "          final_xmax = (temp_xmax + (Amount_to_Expand/1000))/1000\n",
        "        else:\n",
        "          final_xmax = (temp_xmax/1000)\n",
        "        \n",
        "          \n",
        "      #.flextext timecode extraction process\n",
        "      if Timecode_Data_Source == \".flextext\":\n",
        "        for line in time_source:\n",
        "          if \"begin-time-offset=\" in line:\n",
        "            temp_xmin = float(line.split(\"<phrase begin-time-offset=\\\"\")[1].split(\"\\\"\")[0])\n",
        "          if \"end-time-offset=\" in line:    \n",
        "            temp_xmax = float(line.split(\"end-time-offset=\\\"\")[1].split(\"\\\"\")[0])\n",
        "              #Expand timecode data duration\n",
        "            if Expand_Time_Segments == True:\n",
        "              temp_xmin = temp_xmin - (Amount_to_Expand/1000)\n",
        "              temp_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "            try:\n",
        "              tsv_df.at[annotation_counter, 'XMIN'] = temp_xmin / 1000\n",
        "              tsv_df.at[annotation_counter, 'XMAX'] = temp_xmax / 1000\n",
        "              annotation_counter += 1\n",
        "            except KeyError:\n",
        "              print(\"Too many timecode segments...\")\n",
        "        if Expand_Time_Segments == True:\n",
        "          final_xmax = temp_xmax + (Amount_to_Expand/1000)\n",
        "        else:\n",
        "          final_xmax = temp_xmax / 1000\n",
        "\n",
        "  #Add the REFID labels to the DataFrame\n",
        "  total_annotations = annotation_counter - 1\n",
        "  for i in range(annotation_counter):\n",
        "    tsv_df.at[i, 'REFID'] = filename_stem + \"_\" + str(i)\n",
        "  \n",
        "  #Timecode expansion check\n",
        "  if Expand_Time_Segments == True:\n",
        "    annotation_counter = 2\n",
        "    expansion_check = False\n",
        "    while annotation_counter < (total_annotations + 1):\n",
        "      if tsv_df.at[annotation_counter, 'XMIN'] < tsv_df.at[(annotation_counter - 1), 'XMAX'] and expansion_check == False:\n",
        "        print(\"Time expansion error with file \" + filename_stem + \". Reverting to original timecode.\")\n",
        "        expansion_check = True\n",
        "        for i in range(total_annotations):\n",
        "          if i > 0:\n",
        "            tsv_df.at[i, 'XMIN'] = tsv_df.at[i, 'XMIN'] + (Amount_to_Expand/1000)\n",
        "            tsv_df.at[i, 'XMAX'] = tsv_df.at[i, 'XMAX'] - (Amount_to_Expand/1000)\n",
        "      annotation_counter+=1\n",
        "    \n",
        "\n",
        "   \n",
        "#OUTPUT PROCESSES\n",
        "  #Populate new CSV file\n",
        "  with open(new_csv_name, \"w\") as csv_new:\n",
        "    \n",
        "    \n",
        "    annotation_counter = 1\n",
        "    while annotation_counter < (total_annotations - 1):\n",
        "      if Timecode_Data_Source != None:\n",
        "        csv_new.write(tsv_df.at[annotation_counter, 'REFID'] + ',' + str(tsv_df.at[annotation_counter, 'XMIN']) + ',' + str(tsv_df.at[annotation_counter, 'XMAX']))\n",
        "      counter = 1\n",
        "      if Text_Data_Source != None:\n",
        "        while counter < (total_columns + 1):\n",
        "\n",
        "          csv_new.write(',' + tsv_df.at[annotation_counter, column_names[(counter - 1)]]) \n",
        "          counter += 1\n",
        "      csv_new.write('\\n')\n",
        "      annotation_counter += 1\n",
        "    \n",
        "         \n",
        "    \n",
        "    #Populate new .flextext file\n",
        "    if Text_Data_Source != \"None\":\n",
        "      with open(new_flextext_name, \"w\") as new_flextext:\n",
        "        document_uuid = str(uuid.uuid4())\n",
        "        media_uuid = str(uuid.uuid4())\n",
        "        analysis_lg_code = \"en\"\n",
        "        document_title = filename_stem\n",
        "\n",
        "        new_flextext.write(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\")\n",
        "        new_flextext.write(\"<document version=\\\"2\\\">\\n\")\n",
        "        new_flextext.write(\"    <interlinear-text guid=\\\"\" + document_uuid +  \"\\\">\\n\")\n",
        "        new_flextext.write(\"        <item lang=\\\"\" + analysis_lg_code + \"\\\" type=\\\"title\\\">\" + document_title + \"</item>\\n\")\n",
        "        new_flextext.write(\"        <paragraphs>\\n\")\n",
        "        \n",
        "\n",
        "        #Cycle through all of the annotations    \n",
        "        annotation_counter = 1\n",
        "        while annotation_counter < (total_annotations + 1):\n",
        "          paragraph_uuid = str(uuid.uuid4())\n",
        "          phrase_uuid = str(uuid.uuid4())\n",
        "          new_flextext.write(\"            <paragraph guid=\\\"\" + paragraph_uuid + \"\\\">\\n\")\n",
        "          new_flextext.write(\"                <phrases>\\n\")\n",
        "          if Timecode_Data_Source != None:\n",
        "            new_flextext.write(\"                    <phrase begin-time-offset=\\\"\" + str(int(tsv_df.at[annotation_counter,'XMIN']*1000)) + \"\\\"\\n\")\n",
        "            new_flextext.write(\"                        end-time-offset=\\\"\" + str(int(tsv_df.at[annotation_counter, 'XMAX']*1000)) + \"\\\"\\n\")\n",
        "          else:\n",
        "            new_flextext.write(\"                    <phrase\\n\")\n",
        "          new_flextext.write(\"                        guid=\\\"\" + phrase_uuid + \"\\\"\\n\")\n",
        "          new_flextext.write(\"                        media-file=\\\"\" + media_uuid + \"\\\" speaker=\\\"A\\\">\\n\")\n",
        "          for i in range(number_of_columns):\n",
        "            if column_types[i] == \"Transcription\":\n",
        "              new_flextext.write(\"                        <item lang=\\\"\" + column_lgs[i] + \"\\\" type=\\\"txt\\\">\" + str(tsv_df.at[annotation_counter, column_names[i]]) + \"</item>\\n\")\n",
        "          new_flextext.write(\"                        <words/>\\n\")\n",
        "          if \"zxx\" not in column_lgs:\n",
        "            new_flextext.write(\"                        <item lang=\\\"zxx\\\" type=\\\"gls\\\">\" + tsv_df.at[annotation_counter, 'REFID'] + \"</item>\\n\")\n",
        "          for i in range(number_of_columns):\n",
        "            if column_types[i] == \"Translation\":\n",
        "              new_flextext.write(\"                        <item lang=\\\"\" + column_lgs[i] + \"\\\" type=\\\"gls\\\">\" + str(tsv_df.at[annotation_counter, column_names[i]]) + \"</item>\\n\")\n",
        "          new_flextext.write(\"                    </phrase>\\n\")\n",
        "          new_flextext.write(\"                </phrases>\\n\")\n",
        "          new_flextext.write(\"            </paragraph>\\n\")\n",
        "          annotation_counter += 1\n",
        "\n",
        "        new_flextext.write(\"        </paragraphs>\\n\")\n",
        "        new_flextext.write(\"        <media-files offset-type=\\\"\\\">\\n\")\n",
        "        new_flextext.write(\"            <media guid=\\\"\" + media_uuid + \"\\\" location=\\\"\\\"/>\\n\")\n",
        "        new_flextext.write(\"        </media-files>\\n\")\n",
        "        new_flextext.write(\"    </interlinear-text>\\n\")\n",
        "        new_flextext.write(\"</document>\")\n",
        "      new_flextext.close()      \n",
        "\n",
        "    #Populate new .TextGrid file, with three tiers: unique REF ID, transcription, and translation    \n",
        "     \n",
        "    if Timecode_Data_Source != \"None\":   \n",
        "      with open(new_textgrid_name, \"w\") as textgrid_new:                          \n",
        "        #Fill out the beginning of the TextGrid file\n",
        "        textgrid_new.write(\"File type = \\\"ooTextFile\\\"\\nObject class = \\\"TextGrid\\\"\\n\\nxmin = 0\\nxmax = \" + str(final_xmax) + \"\\ntiers? <exists>\\nsize = \" + str((total_columns + 1)) + \"\\nitem []:\\n\")\n",
        "        tier_counter = 1\n",
        "        #Fill out the beginning of each tier in the TextGrid\n",
        "        while tier_counter < (total_columns + 2):                                \n",
        "          textgrid_new.write(\"    item [\" + str(tier_counter) + \"]:\\n\")\n",
        "          textgrid_new.write(\"        class = \\\"IntervalTier\\\"\\n\")\n",
        "          if tier_counter == 1:\n",
        "            textgrid_new.write(\"        name = \\\"REFID\\\"\\n\")\n",
        "          else:\n",
        "            if Text_Data_Source != \"None\":\n",
        "              textgrid_new.write(\"        name = \\\"\" + column_names[(tier_counter - 2)] + \"\\\"\\n\")               \n",
        "          textgrid_new.write(\"        xmin = 0\\n\")\n",
        "          textgrid_new.write(\"        xmax = \" + str(final_xmax) + \"\\n\")\n",
        "          textgrid_new.write(\"        intervals: size = \" + str((total_annotations * 2) + 1) + \"\\n\")\n",
        "          #Fill out each TextGrid interval using the dictionary\n",
        "          annotation_counter = 1\n",
        "          while annotation_counter < (total_annotations + 1):\n",
        "            if annotation_counter == 1:\n",
        "              temp_prev_xmax = 0\n",
        "            else:\n",
        "              temp_prev_xmax = tsv_df.at[(annotation_counter - 1),'XMAX']\n",
        "            temp_next_xmin = tsv_df.at[annotation_counter,'XMIN']\n",
        "            textgrid_new.write(\"        intervals [\" + str(((annotation_counter * 2) - 1)) + \"]:\\n\")\n",
        "            if annotation_counter == 1:\n",
        "              textgrid_new.write(\"            xmin = 0\\n\")\n",
        "            else:\n",
        "              textgrid_new.write(\"            xmin = \" + str(temp_prev_xmax) + \"\\n\")\n",
        "            textgrid_new.write(\"            xmax = \" + str(temp_next_xmin) + \"\\n\")\n",
        "            textgrid_new.write(\"            text = \\\"\\\"\\n\")\n",
        "            textgrid_new.write(\"        intervals [\" + str(annotation_counter * 2) + \"]:\\n\")\n",
        "            textgrid_new.write(\"            xmin = \" + str(tsv_df.at[annotation_counter, 'XMIN']) + \"\\n\")\n",
        "            textgrid_new.write(\"            xmax = \" + str(tsv_df.at[annotation_counter, 'XMAX']) + \"\\n\")\n",
        "            if tier_counter == 1:\n",
        "              textgrid_new.write(\"            text = \\\"\" + str(tsv_df.at[annotation_counter, 'REFID']) + \"\\\"\\n\")\n",
        "            else:\n",
        "              if Text_Data_Source != \"None\":\n",
        "                textgrid_new.write(\"            text = \\\"\" + str(tsv_df.at[annotation_counter, column_names[(tier_counter - 2)]]) + \"\\\"\\n\")\n",
        "              else:\n",
        "                textgrid_new.write(\"            text = \\\"\\\"\\n\")\n",
        "            annotation_counter += 1\n",
        "          textgrid_new.write(\"        intervals [\" + str((((annotation_counter - 1) * 2) + 1)) + \"]:\\n\")\n",
        "          xmax_of_final_annotation = tsv_df.at[total_annotations, 'XMAX']\n",
        "          textgrid_new.write(\"            xmin = \" + str(xmax_of_final_annotation) + \"\\n\")\n",
        "          textgrid_new.write(\"            xmax = \" + str(final_xmax) + \"\\n\")\n",
        "          textgrid_new.write(\"            text = \\\"\\\"\\n\")\n",
        "          tier_counter += 1\n",
        "      textgrid_new.close()                                 \n",
        "\n",
        "#Populate new .EAF file\n",
        "#Fill out the beginning of the EAF file\n",
        "    if Timecode_Data_Source != \"None\":\n",
        "      with open(new_eaf_name, \"w\") as eaf_new:\n",
        "        eaf_new.write(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\")\n",
        "        eaf_new.write(\"<ANNOTATION_DOCUMENT AUTHOR=\\\"unspecified\\\" DATE=\\\"\" + str(now.year) + \"-\" + str(now.month) + \"-\" + str(now.day) + \"T\" + str(now.hour) + \":\" + str(now.minute) + \":\" + str(now.second) + \"-08:00\\\" FORMAT=\\\"3.0\\\" VERSION=\\\"3.0\\\" xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xsi:noNamespaceSchemaLocation=\\\"http://www.mpi.nl/tools/elan/EAFv3.0.xsd\\\">\\n\")\n",
        "        eaf_new.write(\"    <HEADER MEDIA_FILE=\\\"\\\" TIME_UNITS=\\\"milliseconds\\\">\\n\")\n",
        "  #Fixes slashes in Windows directories\n",
        "        if Operating_System == 'Windows':                   \n",
        "          eaf_new.write(\"        <MEDIA_DESCRIPTOR MEDIA_URL=\\\"file:///\" + filename_stem + \".wav\\\" MIME_TYPE=\\\"audio/x-wav\\\" RELATIVE_MEDIA_URL=\\\"./\" + filename_stem + \".wav\\\"/>\\n\")\n",
        "        else:\n",
        "          eaf_new.write(\"        <MEDIA_DESCRIPTOR MEDIA_URL=\\\"file://\" + filename_stem + \".wav\\\" MIME_TYPE=\\\"audio/x-wav\\\" RELATIVE_MEDIA_URL=\\\"./\" + filename_stem + \".wav\\\"/>\\n\")\n",
        "        eaf_new.write(\"        <PROPERTY NAME=\\\"URN\\\">urn:nl-mpi-tools-elan-eaf:93cd58ea-4af9-44d5-a6d5-d468217ccf5e</PROPERTY>\\n\")\n",
        "        eaf_new.write(\"        <PROPERTY NAME=\\\"lastUsedAnnotationId\\\">\" + str((5 * total_annotations)) + \"</PROPERTY>\\n\")\n",
        "        eaf_new.write(\"    </HEADER>\\n\")\n",
        "        eaf_new.write(\"    <TIME_ORDER>\\n\")                       \n",
        "  #Fill out time slots\n",
        "        annotation_counter = 1\n",
        "        while annotation_counter < (total_annotations + 1):\n",
        "          eaf_new.write(\"        <TIME_SLOT TIME_SLOT_ID=\\\"ts\" + str((annotation_counter * 2) - 1) + \"\\\" TIME_VALUE=\\\"\" + str(int(1000 * tsv_df.at[annotation_counter, 'XMIN'])) + \"\\\"/>\\n\")\n",
        "          eaf_new.write(\"        <TIME_SLOT TIME_SLOT_ID=\\\"ts\" + str((annotation_counter * 2)) + \"\\\" TIME_VALUE=\\\"\" + str(int(1000 * tsv_df.at[annotation_counter, 'XMAX'])) + \"\\\"/>\\n\")\n",
        "          annotation_counter += 1\n",
        "  #Fill out RFID annotations\n",
        "        eaf_new.write(\"    </TIME_ORDER>\\n\")\n",
        "        eaf_new.write(\"    <TIER DEFAULT_LOCALE=\\\"en\\\" LINGUISTIC_TYPE_REF=\\\"REFID\\\" TIER_ID=\\\"REFID\\\">\\n\")\n",
        "        annotation_counter = 1\n",
        "        while annotation_counter < (total_annotations + 1):\n",
        "          eaf_new.write(\"        <ANNOTATION>\\n\")\n",
        "          eaf_new.write(\"            <ALIGNABLE_ANNOTATION ANNOTATION_ID=\\\"a\" + str((2 * total_annotations) + annotation_counter) + \"\\\" TIME_SLOT_REF1=\\\"ts\" + str((annotation_counter * 2) -1) + \"\\\" TIME_SLOT_REF2=\\\"ts\" + str(annotation_counter * 2) + \"\\\">\\n\")\n",
        "          eaf_new.write(\"                <ANNOTATION_VALUE>\" + str(tsv_df.at[annotation_counter, 'REFID']) + \"</ANNOTATION_VALUE>\\n\")\n",
        "          eaf_new.write(\"            </ALIGNABLE_ANNOTATION>\\n\")\n",
        "          eaf_new.write(\"        </ANNOTATION>\\n\")\n",
        "          annotation_counter += 1\n",
        "        eaf_new.write(\"    </TIER>\\n\")                \n",
        "  #Fill out transcription and translation annotations\n",
        "        if Text_Data_Source != \"None\":\n",
        "          tier_counter = 1\n",
        "          while tier_counter < total_columns + 1:                        \n",
        "            eaf_new.write(\"    <TIER DEFAULT_LOCALE=\\\"en\\\" LINGUISTIC_TYPE_REF=\\\"\" + column_names[(tier_counter - 1)] + \"\\\" PARENT_REF=\\\"REFID\\\" TIER_ID=\\\"\" + column_names[(tier_counter - 1)] + \"\\\">\\n\")\n",
        "            annotation_counter = 1\n",
        "            while annotation_counter < (total_annotations + 1):\n",
        "              eaf_new.write(\"        <ANNOTATION>\\n\")\n",
        "              eaf_new.write(\"            <REF_ANNOTATION ANNOTATION_ID=\\\"a\" + str(((tier_counter + 2) * total_annotations) + annotation_counter) + \"\\\" ANNOTATION_REF=\\\"a\" + str((2 * total_annotations) + annotation_counter) + \"\\\">\\n\")\n",
        "              eaf_new.write(\"                <ANNOTATION_VALUE>\" + str(tsv_df.at[annotation_counter, column_names[(tier_counter - 1)]]) + \"</ANNOTATION_VALUE>\\n\")\n",
        "              eaf_new.write(\"            </REF_ANNOTATION>\\n\")\n",
        "              eaf_new.write(\"        </ANNOTATION>\\n\")\n",
        "              annotation_counter += 1\n",
        "            eaf_new.write(\"    </TIER>\\n\")\n",
        "            tier_counter += 1\n",
        "                      \n",
        "                                \n",
        "  #Fill out end of the EAF file\n",
        "        eaf_new.write(\"    <LINGUISTIC_TYPE GRAPHIC_REFERENCES=\\\"false\\\" LINGUISTIC_TYPE_ID=\\\"REFID\\\" TIME_ALIGNABLE=\\\"true\\\"/>\\n\")\n",
        "        if Text_Data_Source != \"None\":\n",
        "          counter = 1\n",
        "          while counter < (total_columns + 1):\n",
        "            eaf_new.write(\"    <LINGUISTIC_TYPE CONSTRAINTS=\\\"Symbolic_Association\\\" GRAPHIC_REFERENCES=\\\"false\\\" LINGUISTIC_TYPE_ID=\\\"\" + column_names[(counter - 1)] + \"\\\" TIME_ALIGNABLE=\\\"false\\\"/>\\n\")    \n",
        "            counter += 1\n",
        "        eaf_new.write(\"    <LOCALE COUNTRY_CODE=\\\"US\\\" LANGUAGE_CODE=\\\"en\\\"/>\\n\")\n",
        "        eaf_new.write(\"    <CONSTRAINT DESCRIPTION=\\\"Time subdivision of parent annotation's time interval, no time gaps allowed within this interval\\\" STEREOTYPE=\\\"Time_Subdivision\\\"/>\\n\")\n",
        "        eaf_new.write(\"    <CONSTRAINT DESCRIPTION=\\\"Symbolic subdivision of a parent annotation. Annotations refering to the same parent are ordered\\\" STEREOTYPE=\\\"Symbolic_Subdivision\\\"/>\\n\")\n",
        "        eaf_new.write(\"    <CONSTRAINT DESCRIPTION=\\\"1-1 association with a parent annotation\\\" STEREOTYPE=\\\"Symbolic_Association\\\"/>\\n\")\n",
        "        eaf_new.write(\"    <CONSTRAINT DESCRIPTION=\\\"Time alignable annotations within the parent annotation's time interval, gaps are allowed\\\" STEREOTYPE=\\\"Included_In\\\"/>\\n\")\n",
        "        eaf_new.write(\"</ANNOTATION_DOCUMENT>\")\n",
        "      \n",
        "    print(\"Processing complete for: \" + fn)\n",
        "zip_filename = \"eLDW_Output_\" + now.strftime(\"%Y%m%d\") + \".zip\"\n",
        "!zip -r $zip_filename Output/\n",
        "files.download(zip_filename)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}